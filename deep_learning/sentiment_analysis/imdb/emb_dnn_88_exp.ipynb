{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets,models,layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(1000,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "(trainX,trainy),(testX,testy) = datasets.imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = datasets.imdb.get_word_index()\n",
    "word_index = {k:v+3 for k,v in word_index.items()}\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<UNUSED>'] = 3\n",
    "rev_word_index = dict([v,k] for (k,v) in word_index.items())\n",
    "def decode(sent_ints):\n",
    "    return ' '.join([rev_word_index[i] for i in sent_ints]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=500\n",
    "trainX = keras.preprocessing.sequence.pad_sequences(trainX,\n",
    "                    value=word_index['<PAD>'],\n",
    "                    padding='post',\n",
    "                    maxlen=maxlen)\n",
    "testX = keras.preprocessing.sequence.pad_sequences(testX,\n",
    "                    value=word_index['<PAD>'],\n",
    "                    padding='post',\n",
    "                    maxlen=maxlen)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size,embedding_dim,input_length=maxlen),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(16,activation='relu'),\n",
    "        layers.Dense(1,activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 103us/sample - loss: 0.6919 - accuracy: 0.5599 - val_loss: 0.6900 - val_accuracy: 0.6062\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.6848 - accuracy: 0.6646 - val_loss: 0.6782 - val_accuracy: 0.7172\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.6640 - accuracy: 0.7534 - val_loss: 0.6499 - val_accuracy: 0.7585\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.6236 - accuracy: 0.7840 - val_loss: 0.6047 - val_accuracy: 0.7745\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.5647 - accuracy: 0.8104 - val_loss: 0.5446 - val_accuracy: 0.8110\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s 91us/sample - loss: 0.4994 - accuracy: 0.8384 - val_loss: 0.4876 - val_accuracy: 0.8308\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.4407 - accuracy: 0.8585 - val_loss: 0.4408 - val_accuracy: 0.8457\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.3934 - accuracy: 0.8701 - val_loss: 0.4038 - val_accuracy: 0.8552\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.3560 - accuracy: 0.8805 - val_loss: 0.3766 - val_accuracy: 0.8606\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.3276 - accuracy: 0.8879 - val_loss: 0.3561 - val_accuracy: 0.8669\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 2s 89us/sample - loss: 0.3040 - accuracy: 0.8956 - val_loss: 0.3408 - val_accuracy: 0.8692\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.2849 - accuracy: 0.9018 - val_loss: 0.3282 - val_accuracy: 0.8730\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.2696 - accuracy: 0.9056 - val_loss: 0.3193 - val_accuracy: 0.8754\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.2552 - accuracy: 0.9124 - val_loss: 0.3109 - val_accuracy: 0.8782\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 2s 91us/sample - loss: 0.2431 - accuracy: 0.9154 - val_loss: 0.3041 - val_accuracy: 0.8804\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s 91us/sample - loss: 0.2326 - accuracy: 0.9192 - val_loss: 0.3001 - val_accuracy: 0.8820\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.2223 - accuracy: 0.9237 - val_loss: 0.2951 - val_accuracy: 0.8834\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.2133 - accuracy: 0.9272 - val_loss: 0.2936 - val_accuracy: 0.8837\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 2s 91us/sample - loss: 0.2053 - accuracy: 0.9300 - val_loss: 0.2913 - val_accuracy: 0.8835\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 2s 90us/sample - loss: 0.1979 - accuracy: 0.9316 - val_loss: 0.2888 - val_accuracy: 0.8850\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainX,trainy,validation_data=[testX,testy],\n",
    "                   epochs=20,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Lets get the word embeddings\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(vocab_size):\n",
    "  word = rev_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
